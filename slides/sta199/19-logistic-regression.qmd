---
title: "Models with multiple predictors"
subtitle: "Lecture 19"
date: "November 3, 2022"
format: revealjs
editor_options: 
  chunk_output_type: console
---

# Warm up

## While you wait for class to begin...

-   Clone your `ae-18` project from GitHub, render your document, update your name, and commit and push.
-   Post any questions you have about the material so far at [sli.do / #sta199](https://app.sli.do/event/oKXbMbrd8RMEDfLPgEetpn).

## Announcements

-   Team evaluations open -- due Sat night, 11:59pm (so we can review before Monday's lab)
-   HW 5 will be posted soon
-   HW 6

## Questions from last time {.smaller}

**Q: What is a two-way table?**

A: A table of frequencies for two categorical variables.

. . .

::: question
How do we go from what's on the left to what's on the right?
:::

::: columns
::: {.column width="50%"}
```{r}
#| message: false

library(palmerpenguins)
library(tidyverse)

penguins |>
  count(species, sex)
```
:::

::: {.column width="50%"}
```{r}
#| echo: false

penguins |>
  count(species, sex) |>
  pivot_wider(names_from = sex, values_from = n)
```
:::
:::

## Questions from last time {.smaller}

**Q: How do we build a two-way table in a pipeline?**

. . .

```{r}

penguins |>
  count(species, sex) |>
  pivot_wider(names_from = sex, values_from = n)
```

## Questions from last time {.smaller}

**Q: How do we know whether we can scrape data from a website?**

. . .

A: Technically, with `robotstxt::paths_allowed()`, but that doesn't address ethical considerations which are just as important, if not more.

```{r}
#| warning: false

robotstxt::paths_allowed("https://www.duke.edu/")
```

## Questions from last time {.smaller}

**Q: What is the proper notation for writing out a model?**

. . .

-   Population models (truth):

$$
y = \beta_0 + \beta_1 \times x + \epsilon
$$

-   Sample models (estimated):

$$
\hat{y} = b_0 + b_1 \times x
$$

## Goals

-   Fit and interpret models for predicting binary outcomes

-   Introduce over fitting and mitigating it by splitting the data into training and testing sets

# Logistic regression

## What is logistic regression?

::: columns
::: {.column width="50%"}
-   Similar to linear regression....
    but

-   Modeling tool when our response is categorical
:::

::: {.column width="50%"}
![](images/18/logistic.png){fig-align="center"}
:::
:::

## Modelling binary outcomes

-   Variables with binary outcomes follow the B**ernouilli distribution**:

    -   $y_i \sim Bern(p)$

    -   $p$: Probability of success

    -   $1-p$: Probability of failure

-   We can't model $y$ directly, so instead we model $p$

## Linear model

$$
p_i = \beta_o + \beta_1 \times X_1 + \cdots + \epsilon
$$

-   But remember that $p$ must be between 0 and 1

-   We need a **link function** that transforms the linear model to have an appropriate range

## Logit link function

The **logit** function take values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:

$$
logit(p) = log \bigg( \frac{p}{1 - p} \bigg)
$$

```{r}
#| include: false

library(tidyverse)

tibble(
  x = seq(0.001, 0.999, 0.001),
  y = log(x / (1-x))
) |>
  ggplot(aes(x = x, y = y)) +
  geom_smooth() +
  scale_x_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  labs(x = "p", y = "logit(p)", title = "logit(p) vs. p")
```

## This isn't exactly what we need though.....

-   Recall, the goal is to take values between -$\infty$ and $\infty$ and map them to probabilities.

-   We need the opposite of the link function... or the *inverse*

-   Taking the inverse of the logit function will map arbitrary real values back to the range \[0, 1\]

## Generalized linear model

-   We model the logit (log-odds) of $p$ :

$$
logit(p) = log \bigg( \frac{p}{1 - p} \bigg) = \beta_o + \beta_1 \times X1_i + \cdots + \epsilon 
$$

-   Then take the inverse to obtain the predicted $p$:

$$
p_i = \frac{e^{\beta_o + \beta_1 \times X1_i + \cdots + \epsilon}}{1 + e^{\beta_o + \beta_1 \times X1_i + \cdots + \epsilon}}
$$

## A logistic model visualized

```{r}
#| echo: false

sigmoid = function(x) 1 / (1 + exp(-x + 10))
plot.function(sigmoid, from = 0, to = 20, n = 101, 
              ylab="P(Y = 1)", 
              xlab = "X (predictor)", 
              main = "Predicted probability Y = 1", 
              lwd = 3)
```

## Takeaways

-   Generalized linear models allow us to fit models to predict non-continuous outcomes

-   Predicting binary outcomes requires modeling the log-odds of success, where p = probability of success

## Review questions

::: question
What is a **training** data set?
:::

. . .

"Sandbox" for model building.
Build the model on these data.

. . .

::: question
What is a **testing** data set?
:::

. . .

Held in reserve to test one or two chosen models and to evaluate their performance.
