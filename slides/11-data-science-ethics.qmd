---
title: "Data science ethics"
subtitle: "Lecture 11"
date: "October 4, 2022"
format: revealjs
---

# Warm up

```{r}
#| echo: false

library(countdown)
```

## While you wait for class to begin...

Open your `ae-08` project in RStudio, render your document, and commit and push.

## Announcements {.smaller}

-   ...

# Misrepresentation

## Stand your ground {.smaller}

::: question
In 2005, the [*Florida legislature*](https://en.wikipedia.org/w/index.php?search=Florida%20legislature) passed the controversial ["Stand Your Ground" law](https://en.wikipedia.org/wiki/Stand-your-ground_law) that broadened the situations in which citizens can use lethal force to protect themselves against perceived threats.
Advocates believed that the new law would ultimately reduce crime; opponents feared an increase in the use of lethal force.
:::

::: columns
::: {.column width="40%"}
::: question
What does the following visualization, published by the news service [Reuters](http://static5.businessinsider.com/image/53038b556da8110e5ce82be7-604-756/florida%20gun%20deaths.jpg) on February 16, 2014, say about the number of firearm murders in Florida over the years?
:::
:::

::: {.column width="60%"}
![](images/11/stand-your-ground.png){fig-align="center"}
:::
:::

::: aside
Source: [MDSR, Chp 8](https://mdsr-book.github.io/mdsr2e/ch-ethics.html).
:::

## COVID cases {.smaller}

::: question
In May 2020, the state of Georgia published the following visualization.
What does this plot say about the number of COVID cases in the most impacted counties?
:::

![](images/11/georgia-covid-deaths.png){fig-align="center" width="700"}

## `ae-08`

...

# Algorithmic bias

## Two examples

-   Criminal justice: [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) (ProPublica, 2016)

-   Language models: [Stochastic Parrots](https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf) (Bender et. al., 2021)

## Machine Bias {.smaller}

2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:

::: columns
::: {.column width="70%"}
> In forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.
>
> -   The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.
>
> -   White defendants were mislabeled as low risk more often than black defendants.
:::

::: {.column width="30%"}
![](images/11/machine-bias.png){fig-align="center" width="800"}
:::
:::

::: aside
Source: [ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
:::

## Risk score errors

::: columns
::: {.column width="35%"}
::: question
What is common among the defendants who were assigned a high/low risk score for reoffending?
:::
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](images/11/machine-bias-petty-theft-1.png){fig-align="center" width="300" height="250"} ![](images/11/machine-bias-petty-theft-2.png){fig-align="center" width="300" height="180"}
:::

::: {.column width="30%"}
![](images/11/machine-bias-drug-posession-1.png){fig-align="center" width="300" height="250"} ![](images/11/machine-bias-drug-posession-2.png){fig-align="center" width="300" height="180"}
:::
:::

## Risk scores

::: columns
::: {.column width="35%"}
::: question
How can an algorithm that doesn't use race as input data be racist?
:::
:::

::: {.column width="5%"}
:::

::: {.column width="60%"}
![](images/11/machine-bias-risk-scores.png){fig-align="center" width="480"}
:::
:::

## Goggle translate

![](images/11/google-translate.png){fig-align="center"}

::: aside
Source: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)
:::

## 
