{
  "hash": "fd4c2a3eb19970af1792eba1413cee4a",
  "result": {
    "markdown": "---\ntitle: \"Data science ethics\"\nsubtitle: \"Lecture 11\"\ndate: \"October 4, 2022\"\nformat: revealjs\n---\n\n\n# Warm up\n\n\n::: {.cell}\n\n:::\n\n\n## While you wait for class to begin...\n\nOpen your `ae-08` project in RStudio, render your document, and commit and push.\n\n## Announcements {.smaller}\n\n-   ...\n\n# Misrepresentation\n\n## Stand your ground {.smaller}\n\n::: question\nIn 2005, the [*Florida legislature*](https://en.wikipedia.org/w/index.php?search=Florida%20legislature) passed the controversial [\"Stand Your Ground\" law](https://en.wikipedia.org/wiki/Stand-your-ground_law) that broadened the situations in which citizens can use lethal force to protect themselves against perceived threats.\nAdvocates believed that the new law would ultimately reduce crime; opponents feared an increase in the use of lethal force.\n:::\n\n::: columns\n::: {.column width=\"40%\"}\n::: question\nWhat does the following visualization, published by the news service [Reuters](http://static5.businessinsider.com/image/53038b556da8110e5ce82be7-604-756/florida%20gun%20deaths.jpg) on February 16, 2014, say about the number of firearm murders in Florida over the years?\n:::\n:::\n\n::: {.column width=\"60%\"}\n![](images/11/stand-your-ground.png){fig-align=\"center\"}\n:::\n:::\n\n::: aside\nSource: [MDSR, Chp 8](https://mdsr-book.github.io/mdsr2e/ch-ethics.html).\n:::\n\n## COVID cases {.smaller}\n\n::: question\nIn May 2020, the state of Georgia published the following visualization.\nWhat does this plot say about the number of COVID cases in the most impacted counties?\n:::\n\n![](images/11/georgia-covid-deaths.png){fig-align=\"center\" width=\"700\"}\n\n## `ae-08`\n\n...\n\n# Algorithmic bias\n\n## Two examples\n\n-   Criminal justice: [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) (ProPublica, 2016)\n\n-   Language models: [Stochastic Parrots](https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf) (Bender et. al., 2021)\n\n## Machine Bias {.smaller}\n\n2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:\n\n::: columns\n::: {.column width=\"70%\"}\n> In forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.\n>\n> -   The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\n>\n> -   White defendants were mislabeled as low risk more often than black defendants.\n:::\n\n::: {.column width=\"30%\"}\n![](images/11/machine-bias.png){fig-align=\"center\" width=\"800\"}\n:::\n:::\n\n::: aside\nSource: [ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n:::\n\n## Risk score errors\n\n::: columns\n::: {.column width=\"35%\"}\n::: question\nWhat is common among the defendants who were assigned a high/low risk score for reoffending?\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"30%\"}\n![](images/11/machine-bias-petty-theft-1.png){fig-align=\"center\" width=\"300\" height=\"250\"} ![](images/11/machine-bias-petty-theft-2.png){fig-align=\"center\" width=\"300\" height=\"180\"}\n:::\n\n::: {.column width=\"30%\"}\n![](images/11/machine-bias-drug-posession-1.png){fig-align=\"center\" width=\"300\" height=\"250\"} ![](images/11/machine-bias-drug-posession-2.png){fig-align=\"center\" width=\"300\" height=\"180\"}\n:::\n:::\n\n## Risk scores\n\n::: columns\n::: {.column width=\"35%\"}\n::: question\nHow can an algorithm that doesn't use race as input data be racist?\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"60%\"}\n![](images/11/machine-bias-risk-scores.png){fig-align=\"center\" width=\"480\"}\n:::\n:::\n\n## Goggle translate\n\n![](images/11/google-translate.png){fig-align=\"center\"}\n\n::: aside\nSource: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)\n:::\n\n## \n",
    "supporting": [
      "11-data-science-ethics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}